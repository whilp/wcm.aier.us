#!/usr/bin/env python

import logging
import os
import sys

from boto.s3.connection import S3Connection

try:
    NullHandler = logging.NullHandler
except AttributeError:
    class NullHandler(logging.Handler):
        def emit(self, record): pass

log = logging.getLogger(__name__)
log.addHandler(NullHandler())

def main():
    paths = (os.path.normpath(p.rstrip("\n")) for p in sys.stdin)
    bucketname = sys.argv[1]

    log.debug("connecting to S3")
    conn = S3Connection()
    bucket = conn.get_bucket(bucketname)

    bucketsync(bucket, paths)

def bucketsync(bucket, paths, delete=True):
    keys = dict((k.name, k) for k in bucket.list())
    log.debug("found %d active keys in bucket %r", len(keys), bucket.name)

    for path in paths:
        key = keys.pop(path, None)
        if not key:
            log.debug("creating key %r", path)
            key = bucket.new_key(path)

        with open(path, 'rb') as fp:
            synckey(key, fp)

    if delete:
        # Any leftover keys have been removed locally and should be deleted from S3.
        for key in keys.values():
            log.debug("deleting key %r", path)
            key.delete()

def synckey(key, fp, public=True, **kwargs):
    name = key.name
    md5 = None
    if key.etag:
        etag = key.etag.strip("\"'")
        (md5, base64md5) = key.compute_md5(fp)
        if etag != md5:
            log.debug("key %r: local md5sum %s does not match remote etag %s",
                    name, md5, etag)
        else:
            log.debug("key %r is unchanged", name)
            # Local and S3 copies match.
            return

    log.debug("uploading key %r", name)
    key.set_contents_from_file(fp, md5=(md5, base64md5))
    if public:
        log.debug("making key %r public", name)
        key.make_public()

if __name__ == "__main__":
    try:
        ret = main()
    except KeyboardInterrupt:
        ret = None
    sys.exit(ret)
